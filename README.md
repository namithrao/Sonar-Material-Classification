# Sonar  Material Classification

## Preprossessing/Data collection
Data collected from: https://www.kaggle.com/datasets/rupakroy/sonarcsv
There was not much preprocessing to do. First I loaded the dataset in using pandas, as that was the most easy to work with, as opposed to pyspark. After loading, I separate labels and feature data into X and y datasets. The label column is encoded using label encoder in order to convert R "rock" and M "metal" to numerical features. The dataset is then split 80/20 train/test respectivley. These are then converted from pandas df to PyTorch Tensor for the coming neural network creation.

## Model Architecture
I use two fully connected layers with 120 and 60 neurons. The dataset has 60 features, so I chose to have one layer 60->120 in order to represent more complex underlying patterns in the data. I apply batch norm layer in order to stabilize the learning rate by normalizing the input to the next layer. I saw from the tensorboard that the was very choppy. This means the learning rate was too unstable. I used dropout layers to reduce overfitting, as my training rate would be 100%, while validation would stay at 80%. I followed that with a linear layer 120->60 which repeats the same process as before. The output is a linear layer that corresponds to the two classes. I used CrossEntropyLoss which is effective with classification with two or more classes. I used adam optimizer because of the adaptive learning rate. when I tried gradient descent optimization, the learning rate did not fit as well. I increased the number of epochs and decreased the learning rate because I wanted to give it a longer time to stabilize. I measured the loss and accuracy on training and validation sets and printed each on each epoch, and then used tensorboard to visualize these metric graphically. The best model was selected based on the highest validation accuracy, noticing if the training accuracy was noticabely higher than validation. This would be a sign of overfitting because the high training accuracy means the model is "memorizing" the data. I adjusted the dropout rate, increased reduces overfitting by introducing noise into the dataset. I attempted to use weight decay to reduce the sharp increases and decreases i noticed at the end.
